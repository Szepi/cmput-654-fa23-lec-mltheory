\documentclass[twoside]{article}

% We add packages, macros here:
\include{header}

%%
%% ADD PACKAGES here:
%%
%
%\usepackage{amsmath,amsfonts,graphicx}
%
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%

\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CMPUT 654 Fa 23: Theoretical Foundations of Machine Learning \hfill Fall 2023} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent {\bf Note}: {\it 
   \LaTeX\  template courtesy of UC Berkeley EECS dept. (\href{https://inst.eecs.berkeley.edu/~cs294-8/sp03/Materials/}{link} to directory)
   }

   \noindent {\bf Disclaimer}: {\it These notes have \underline{\textbf{not}} been subjected to the
   usual scrutiny reserved for formal publications. They may be
   distributed outside this class only with the permission of the
   Instructor.} \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
%%%%%%%%% (To avoid bibliography problems, for now we redefine the \cite command.)
%%%%%%%%% Also commands that create a suitable format for the reference list.
%%%%%%%%\renewcommand{\cite}[1]{[#1]}
%%%%%%%%\def\beginrefs{\begin{list}%
%%%%%%%%        {[\arabic{equation}]}{\usecounter{equation}
%%%%%%%%         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%%%%%%%%         \setlength{\labelwidth}{1.6truecm}}}
%%%%%%%%\def\endrefs{\end{list}}
%%%%%%%%\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.

\include{theorems}

%\newtheorem{theorem}{Theorem}[lecnum]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\myE{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{September 5}{Csaba Szepesv\'ari}{Zixin Zhong}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{About this class}

\begin{enumerate}
    \item Textbook: 'Mathematical Analysis of Machine Learning Algorithms' by Tong Zhang
(\url{https://tongzhang-ml.org/lt-book/lt-book.pdf}). \\
This class may cover 1 chapter per week, and just cover the material up to the `sequential learning' part.
    \item The focus of this class will be {\bf the statistical approach to supervised learning}.
    \item Recording: \url{https://www.youtube.com/watch?v=arbGdCqn2Io}
\end{enumerate}


\section{Supervised learning: a statistical approach}
We start with a few definitions. 

\begin{definition} [Data set]
    Let the data set $D_n = \left( (X_1,Y_1), \ldots, (X_n,Y_n)  \right)$, where $(X_i,Y_i)$'s are i.i.d.\ random variables and $X_i\in \mathcal{X}$, $Y_i\in \mathcal{Y}$.
\end{definition}

\begin{remark}[Sets?] \mbox{}
\begin{enumerate}[(a)]
\item  $D_n$ is actually a list of data, but we just say it is a `set' for no good reason; the order of pairs can be important and there can be multiple identical pairs. 
\item An example of $\mathcal{X}$ and $\mathcal{Y}$ is $\mathcal{X}=\mathbb{R}^d$ and $\mathcal{Y}=\mathbb{R}$. 
\end{enumerate}
\end{remark}

\begin{definition}[Predictor, loss function, expected loss]
    Fix a function $f: \mathcal{X}\rightarrow \mathcal{Y}$.
	The expected loss of $f$ is 
    \begin{align*}
        L(f) = \mathbb{E}[ \ell(f(X),Y) ]\,, %= \int \ell(f(x),y) P_{X,Y}(\mathrm{d}x, \mathrm{d}y).
    \end{align*}
    where $(X,Y)$ has the same distribution as $(X_1,Y_1)$.
\end{definition}

\begin{remark}\mbox{}
\begin{enumerate}[(a)]
\item We usually have loss function $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, \infty)$.
\item The quality of a predictor is going to be measured by its expected loss.
\item Data $(X,Y)$ is identically distributed to the data we have during training.
This can be thought of as the ``test data''.
\end{enumerate}
\end{remark}

%    Let  with $(X,Y) \overset{\text{distr}}{\sim} (X_1,Y_1)$ and $(X,Y) \perp D_n$,
%    the loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$,
%    and 
%    the loss of predictor $f$

\begin{remark}[Probability spaces hiding in plain sight]
We should have started by saying: Let us fix a probability space
$(\Omega, \mathcal{F}, \mathbb{P})$.
In what follows, all random variables, unless otherwise specified, will be above this space.
We also use random variable in a liberal fashion: We actually mean random elements.
In what follows, the expectation $\mathbb{E}[\cdot]$ will correspond to the probability measure
$\mathbb{P}$.
\end{remark}


\begin{definition}[Pushforward distributions]
For a random variable
$Z:\Omega \rightarrow \mathcal{Z}$,  we use $P_Z$ denote the push-forward of $Z$:
If $\cF_Z\subset 2^{\mathcal{Z}}$ is the sigma-algebra that makes $\mathcal{Z}$ a measurable space,
for $A\in \mathcal{Z}$,
$P_Z(A)$ is defined by 
\begin{align*}
P_Z(A) = \mathbb{P}( Z^{-1}(A) ) (=\Prob{Z\in A})\,,.
\end{align*}
\end{definition}
 
\begin{definition}
    Let $\mathcal{A}: (\mathcal{X}\times \mathcal{Y})^n \rightarrow \mathcal{Y}^\mathcal{X}$ denote a learning algorithm. Let $f_n=\mathcal{A}(D_n)$ and $L(f_n): \Omega \rightarrow \mathbb{R}$.
\end{definition}

\begin{remark}\mbox{}
\begin{enumerate}[(a)]
\item  $\mathcal{A}$ takes a data set as input and returns predictions as output.
\item $\mathcal{A}$ has to be a measurable function. We are going to assume that all functions are measurable. 
\item Since $D_n$ is random, $f_n$ is random.
\end{enumerate}
\end{remark}


\begin{proposition} 
Assume that $(X,Y)$ is independent of $D_n$ (Notation: $(X,Y) \perp D_n$). Then,
\begin{align*}
    & \mathbb{E}[ \ell \left( f_n(X),Y \right) | D_n ] = L(f_n)\text{ w.p.1  and } \quad
    \mathbb{E}[ \ell \left( f_n(X),Y \right) ] = \mathbb{E} [L(f_n)]\,
\end{align*}
\end{proposition}
\begin{proof}
    (\underline{Homework \#1})
\end{proof}

Let $P$ denote $P_{X,Y}$ for brevity. We will call $P$ a learning \textbf{instance}: $P$ is what is unknown and it summarizes everything about the data.

Note that $L(f)$ depends on $P$. To denote this dependence, by abusing notation, we redefine
$L$ to mark this dependence:
\begin{align*}
L(P,f) = \EE{ \ell(f(X),Y) }\,.
\end{align*}
Note that
\begin{align*}
L(P,f) = \int \ell(f(x),y) \, P(dx,dy)\,.
\end{align*}

\begin{definition}[Expected loss of an algorithm]
The expected loss  of $\mathcal{A}$ on instance $P$ is 
\begin{align*}
L(P,\mathcal{A} ) = \EE{L(P,f_n)}\,.
\end{align*}
\end{definition}

The expected loss $L(\mathcal{A},P ) = \mathbb{E} [L_{P }(f_n)]$. 
Our {\bf goal} is to find an algorithm $\mathcal{A}$ such that $L(\mathcal{A}, P )$ is small no matter what $P$ is.

For every $P$, we try to find a predictor such that the loss is small. How closely can an algorithm approaches the minimum loss?

\noindent 
\underline{Homework \#2}: Is there a single algorithm $\mathcal{A}$ that minimizes $L(\mathcal{A},P)$ for all $P$?

\end{document}