\documentclass[twoside]{article}

% We add packages, macros here:
\include{header}

%%
%% ADD PACKAGES here:
%%
%
%\usepackage{amsmath,amsfonts,graphicx}
%
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%

\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CMPUT 654 Fa 23: Theoretical Foundations of Machine Learning \hfill Fall 2023} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent {\bf Note}: {\it 
   \LaTeX\  template courtesy of UC Berkeley EECS dept. (\href{https://inst.eecs.berkeley.edu/~cs294-8/sp03/Materials/}{link} to directory)
   }

   \noindent {\bf Disclaimer}: {\it These notes have \underline{\textbf{not}} been subjected to the
   usual scrutiny reserved for formal publications. They may be
   distributed outside this class only with the permission of the
   Instructor.} \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
%%%%%%%%% (To avoid bibliography problems, for now we redefine the \cite command.)
%%%%%%%%% Also commands that create a suitable format for the reference list.
%%%%%%%%\renewcommand{\cite}[1]{[#1]}
%%%%%%%%\def\beginrefs{\begin{list}%
%%%%%%%%        {[\arabic{equation}]}{\usecounter{equation}
%%%%%%%%         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%%%%%%%%         \setlength{\labelwidth}{1.6truecm}}}
%%%%%%%%\def\endrefs{\end{list}}
%%%%%%%%\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.

\include{theorems}

%\newtheorem{theorem}{Theorem}[lecnum]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\myE{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{September 5}{Csaba Szepesv\'ari}{Zixin Zhong}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.
This lecture's notes illustrate some uses of
various \LaTeX\ macros.  
Take a look at this and imitate.

\section{About this class}

\begin{enumerate}
    \item Textbook: 'Mathematical Analysis of Machine Learning Algorithms' by Tong Zhang
(\url{https://tongzhang-ml.org/lt-book/lt-book.pdf}). \\
This class may cover 1 chapter per week, and just cover the material up to the `sequential learning' part.
    \item The focus of this class will be {\bf the statistical approach to supervised learning}.
    \item Recording: \url{https://www.youtube.com/watch?v=arbGdCqn2Io}
\end{enumerate}


\section{Supervised learning: a statistical approach}
We first define some basic notations.

\begin{definition} [Data set]
    Let the data set $D_n = \left( (X_1,Y_1), \ldots, (X_n,Y_n)  \right)$, where $(X_i,Y_i)$'s are i.i.d.\ random variables and $X_i\in \mathcal{X}$, $Y_i\in \mathcal{Y}$.
\end{definition}
Remarks: (a) $D_n$ is actually a list of data, but we just say it is a `set' with no good reason; the order of pairs can be important and there can be multiple identical pairs. (b) An example of $\mathcal{X}$ and $\mathcal{Y}$ is $\mathcal{X}=\mathbb{R}^d$ and $\mathcal{Y}=\mathbb{R}$. 

\begin{definition}[Predictor and loss function]
    Let the predictor function $f: \mathcal{X}\rightarrow \mathcal{Y}$ with $(X,Y) \overset{\text{distr}}{\sim} (X_1,Y_1)$ and $(X,Y) \perp D_n$,
    the loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$,
    and 
    the loss of predictor $f$
    \begin{align*}
        L_p(f) = \mathbb{E}[ l(f(X),Y) ] = \int l(f(x),y) P_{X,Y}(\mathrm{d}x, \mathrm{d}y).
    \end{align*}
\end{definition}

Remarks: (a) We usually have loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, \infty)$.
(b) The predictor is going to be measured by its expected loss.
(c) Data $(X,Y)$ is identically distributed to the data we have during training.

\begin{definition}[Probability space]
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.\\
    Let $Z:\Omega \rightarrow \mathcal{Z}$, $P_Z$ denote the (push backward) distribution of random variable $Z$, and $\mathcal{F}\subset 2^\mathcal{Z}$ be a set of measurable subsets of $\mathcal{Z}.$
    Let $\mathbb{P}: \mathcal{F}_0 \rightarrow [0,1]$ (push forward measure). For $A\in \mathcal{F}$, $P_Z(A)=\mathbb{P}(Z\in A) \in [0,1]$.
\end{definition}
 
\begin{definition}
    Let $\mathcal{A}: (\mathcal{X}\times \mathcal{Y})^n \rightarrow \mathcal{Y}^\mathcal{X}$ denote a learning algorithm. Let $f_n=\mathcal{A}(D_n)$ and $L(f_n): \Omega \rightarrow \mathbb{R}$.
\end{definition}

Remarks:
 (a) $\mathcal{A}$ takes a data set as input and returns predictions as output.
 (b) $\mathcal{A}$ has to be a measurable function.
 (c) Since $D_n$ is random, $f_n$ is random.

\begin{lemma} Let $P$ denote $P_{X,Y}$ for simplicity.
\begin{align*}
    & \mathbb{E}[ l\left( f_n(X),Y \right) | D_n ] = L_p(f_n),\quad 
    \mathbb{E}[ l\left( f_n(X),Y \right) ] = \mathbb{E} [L_{P }(f_n)].
\end{align*}
\end{lemma}
\begin{proof}
    (\underline{Homework \#1})
\end{proof}

Fix $n$, we define the expected loss $L(\mathcal{A},P ) = \mathbb{E} [L_{P }(f_n)]$. Our {\bf goal} is to find an algorithm $\mathcal{A}$ such that $L(\mathcal{A}, P )$ is small no matter what $P$ is.

\underline{Homework \#2}: Is there a single algorithm $\mathcal{A}$ that minimizes $L(\mathcal{A},P)$ for all $P$?

Remarks: For every $P$, we try to find a predictor such that the loss is small. How closely can an algorithm approaches the minimum loss?
